{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "z3bHeqjCsqQK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samortchy/personal-AI-projects/blob/main/Video-To-Text%20Summrization/Video_Summrization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Enhanced cleaning for abstractive models (preserves context)\n",
        "# def clean_text(text):\n",
        "#     text = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', text)  # Remove brackets/parentheses\n",
        "#     text = re.sub(r'\\s+', ' ', text).strip()     # Fix whitespace\n",
        "#     return text\n",
        "\n",
        "# df_english['article'] = df_english['article'].apply(clean_text)\n",
        "# df_english['highlights'] = df_english['highlights'].apply(clean_text)\n",
        "\n",
        "# df_english.head()"
      ],
      "metadata": {
        "id": "3_TtmBshjLNe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1\n",
        "\n",
        "# Download tokenizer data\n",
        "nltk.download('punkt', download_dir='/root/nltk_data')\n",
        "nltk.data.path.append('/root/nltk_data')\n",
        "\n",
        "# Download stopwords\n",
        "# nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "kn8xA3hhlZH1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-cache-dir numpy camel-tools"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0y6-2hLN8uo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install camel-tools"
      ],
      "metadata": {
        "id": "g0FHM3ys5dA8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BQzZMXnRfmxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "acf02Jc3Lgoz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall sentence-transformers transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jieCl1S8fxzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Use cu118 for Colab\n",
        "\n",
        "# Reinstall transformers and sentence-transformers to ensure they link correctly with the new torch installation\n",
        "!pip install --upgrade --force-reinstall transformers\n",
        "!pip install --upgrade --force-reinstall sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sELKbx8fhHop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "# Explicitly import PreTrainedModel from modeling_utils\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.morphology.analyzer import Analyzer\n",
        "import stanza\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "CAsTH97PE1X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ALL NLTK data\n",
        "import nltk\n",
        "try:\n",
        "    nltk.download('all', download_dir='/root/nltk_data')\n",
        "    print(\"NLTK 'all' data downloaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK 'all' data: {e}\")\n",
        "\n",
        "# Ensure the path is included\n",
        "nltk.data.path.append('/root/nltk_data')"
      ],
      "metadata": {
        "id": "q5iIConhuyek",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gngQxKkegm2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Collection**"
      ],
      "metadata": {
        "id": "47EGY4lbEhYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title arabic dataset\n",
        "\n",
        "splits = {'train': 'data/train-00000-of-00001.parquet', 'validation': 'data/validation-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
        "df_arabic = pd.read_parquet(\"hf://datasets/BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset/\" + splits[\"train\"])\n",
        "df_arabic_test = pd.read_parquet(\"hf://datasets/BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset/\" + splits[\"test\"])\n",
        "df_arabic_val = pd.read_parquet(\"hf://datasets/BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset/\" + splits[\"validation\"])\n",
        "\n",
        "df_arabic = pd.concat([df_arabic, df_arabic_test, df_arabic_val], axis=0)"
      ],
      "metadata": {
        "id": "k_NN62c4EpmG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title English dataset\n",
        "!unzip '/content/drive/Shareddrives/data science/englishDataset.zip'"
      ],
      "metadata": {
        "id": "eRzE8hhmKxiQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_english = pd.read_csv('cnn_dailymail/train.csv', nrows=20000)"
      ],
      "metadata": {
        "id": "YrzQ4k_egNYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Exploration**"
      ],
      "metadata": {
        "id": "jeda08qsK2OU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Helper functions**"
      ],
      "metadata": {
        "id": "z3bHeqjCsqQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title A function to find all the unique chars in the two datasets\n",
        "def find_unique_chars(df, feature_name):\n",
        "  # Combine all text into one large string\n",
        "  all_text = \" \".join(df[feature_name].astype(str).values)\n",
        "\n",
        "  # Extract all characters that are NOT:\n",
        "  # - English letters (a-z, A-Z)\n",
        "  # - Arabic letters (\\u0621-\\u064A)\n",
        "  # - Digits (0-9)\n",
        "  # - Whitespace\n",
        "  allowed_chars = re.findall(r\"[^\\w\\s\\u0621-\\u064A]\", all_text)\n",
        "\n",
        "  char_counts = Counter(allowed_chars)\n",
        "\n",
        "  for char, count in char_counts.most_common(300):\n",
        "      print(f\"'{char}': {count}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XrsMX46LszdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title A function to detect the number of emojis\n",
        "\n",
        "def detect_emojis_count(df,feature_name):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "    u\"\\U00002700-\\U000027BF\"  # other symbols\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    \"]+\", flags=re.UNICODE)\n",
        "\n",
        "  return df[feature_name].apply(lambda x: bool(emoji_pattern.search(str(x)))).sum()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "o5NNWWCItEwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # @title chars that are repeated or spammed i.e(!!!!)\n",
        "\n",
        "# def garbage_chars(df, feature_name=\"text\"):\n",
        "#     repeated_garbage_pattern = re.compile(r\"([^\\w\\s\\u0621-\\u064A])\\1{2,}\")\n",
        "\n",
        "#     garbage_mask = df[feature_name].apply(lambda x: bool(repeated_garbage_pattern.search(str(x))))\n",
        "\n",
        "#     count = garbage_mask.sum()\n",
        "#     print(\"Number of rows with repeated garbage characters:\", count)\n",
        "\n",
        "#     for text in df.loc[garbage_mask, feature_name]:\n",
        "#         matches = repeated_garbage_pattern.findall(str(text))\n",
        "#         print(f\"Text: {text}\")\n",
        "#         print(f\"Repeated garbage chars found: {matches}\")\n",
        "#         print(\"----\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Gl5D5zfny-UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Arabic Dataset**"
      ],
      "metadata": {
        "id": "nWffsjp7gWp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " df_arabic.head()"
      ],
      "metadata": {
        "id": "D7-joUS5BKUg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_arabic.shape)\n",
        "print(df_arabic_test.shape)\n",
        "print(df_arabic_val.shape)"
      ],
      "metadata": {
        "id": "8dFX6wMMBN_N",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Drop any feature rather thann the text , summary in arabic dataset\n",
        "\n",
        "df_arabic = df_arabic[['text', 'summary']]\n",
        "df_arabic_test = df_arabic_test[['text', 'summary']]\n",
        "df_arabic_val = df_arabic_val[['text', 'summary']]"
      ],
      "metadata": {
        "id": "yns8w8TljW3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_arabic.describe()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u4CJRNG-f0p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_arabic.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wcAF_a6bgQeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_arabic.isnull().sum()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RzxDgSU2hAI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_count_text = detect_emojis_count(df_arabic,\"text\")\n",
        "emoji_count_summary = detect_emojis_count(df_arabic,\"summary\")\n",
        "\n",
        "print(f\"Number of rows with emojis in 'text': {emoji_count_text}\")\n",
        "print(f\"Number of rows with emojis in 'summary': {emoji_count_summary}\")"
      ],
      "metadata": {
        "id": "raehjnL7o_2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_unique_chars(df_arabic,\"text\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Hug1RGk3rlh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_unique_chars(df_arabic,\"summary\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bCwFhPOLuVRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **English Dataset**"
      ],
      "metadata": {
        "id": "qZEYFoHfgjgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " df_english.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DVZXdwRrhQXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_english.shape"
      ],
      "metadata": {
        "id": "rlcfpY3xhq5a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_english.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "9YGG9pXUTxw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_english.describe()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lfosBjRqfH6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_english.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ySfU56ajhwIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_english.isnull().sum()"
      ],
      "metadata": {
        "id": "o2pP_4Odi6vo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_count_article = detect_emojis_count(df_english,\"article\")\n",
        "emoji_count_highlights = detect_emojis_count(df_english,\"highlights\")\n",
        "\n",
        "print(f\"Number of rows with emojis in 'article': {emoji_count_article}\")\n",
        "print(f\"Number of rows with emojis in 'highlights': {emoji_count_highlights}\")"
      ],
      "metadata": {
        "id": "5d3jA8ugqCa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_unique_chars(df_english,\"article\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pSRPXYlbsc6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_unique_chars(df_english,\"highlights\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eXkssbukxsOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "ZrBmoB22h-ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Helper functions**"
      ],
      "metadata": {
        "id": "OPwIbdwz2Tz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title removes emojis\n",
        "\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        u\"\\U00002700-\\U000027BF\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub('', text)"
      ],
      "metadata": {
        "id": "jCG29tJh1WLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title removes all unique chars under 500 occurences\n",
        "\n",
        "def remove_rare_unique_chars(df, feature_name):\n",
        "    all_text = \" \".join(df[feature_name].astype(str).values)\n",
        "    allowed_chars = re.findall(r\"[^\\w\\s\\u0621-\\u064A]\", all_text)\n",
        "    char_counts = Counter(allowed_chars)\n",
        "\n",
        "    for char, count in char_counts.items():\n",
        "        if count <= 500:\n",
        "            df[feature_name] = df[feature_name].astype(str).apply(lambda x: x.replace(char, ''))\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Vh3WgA8K3puC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title handles foreign names in arabic dataset\n",
        "\n",
        "def normalize_foreign_tokens(text):\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n",
        "    text = re.sub(r'\\b[a-zA-Z][a-zA-Z0-9_]{1,}\\b', '<NAME>', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "bmuK00WdMCB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title normalization in  arabic dataset\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(r\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(r\"ى\", \"ي\", text)\n",
        "    text = re.sub(r\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(r\"ئ\", \"ء\", text)\n",
        "    text = re.sub(r\"ً|ٌ|ٍ|َ|ُ|ِ|ّ|ْ\", \"\", text)\n",
        "    text = re.sub(r\"ـ\", \"\", text)\n",
        "    text = re.sub(r\"ه\", \"ة\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "iw6W120hNKzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title arabic dataset lemmatizer\n",
        "\n",
        "def download_arabic_model():\n",
        "    stanza.download('ar')\n",
        "\n",
        "# Initialize pipeline once and reuse it\n",
        "nlp_arabic = stanza.Pipeline('ar', processors='tokenize,pos,lemma', use_gpu=False)\n",
        "\n",
        "def lemmatize_arabic_text(text: str) -> str:\n",
        "    doc = nlp_arabic(text)\n",
        "    lemmas = [word.lemma for sentence in doc.sentences for word in sentence.words]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "# Example usage:\n",
        "#download_arabic_model()  # Run once, comment after #the model is downloaded don't re-download except if it is missing\n",
        "# print(lemmatize_arabic_text(\"الطلاب يذهبون إلى المدرسة\"))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_UikDfnTezRn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nltk_pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'): return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'): return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'): return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'): return wordnet.ADV\n",
        "    else: return wordnet.NOUN"
      ],
      "metadata": {
        "id": "XjdLVs5QMddG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # @title arabic dataset stemming\n",
        "# # @title arabic dataset stemmer\n",
        "\n",
        "# import stanza\n",
        "# from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "\n",
        "# def download_arabic_model():\n",
        "#     stanza.download('ar')\n",
        "\n",
        "# # Initialize pipeline once and reuse it\n",
        "# nlp_arabic = stanza.Pipeline('ar', processors='tokenize,pos,lemma', use_gpu=False)\n",
        "\n",
        "# def stem_arabic_text(text: str) -> str:\n",
        "#     \"\"\"\n",
        "#     Stems Arabic text using Stanza.\n",
        "#     \"\"\"\n",
        "#     doc = nlp_arabic(text)\n",
        "#     stems = [word.lemma for sentence in doc.sentences for word in sentence.words]\n",
        "#     return ' '.join(stems)\n",
        "\n",
        "# # Example usage:\n",
        "# #download_arabic_model() # Run this line once to download the model\n",
        "# print(stem_arabic_text(\"الطلاب يذهبون إلى المدرسة\"))\n"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "DX0ErpdqmMfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # @title english dataset lemmatizer\n",
        "\n",
        "def lemmatize_english_text(tokens):\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word, nltk_pos_tagger(pos)) for word, pos in pos_tags]"
      ],
      "metadata": {
        "id": "WgLWSBM2oHX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # @title english dataset stemming\n",
        "\n",
        "# # @title english dataset stemmer\n",
        "\n",
        "# from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
        "\n",
        "# def stem_english_text(tokens, stemmer_type='porter'):\n",
        "#     \"\"\"\n",
        "#     Stems English text using different NLTK stemmers.\n",
        "\n",
        "#     Args:\n",
        "#         tokens: List of tokens (words).\n",
        "#         stemmer_type: Type of stemmer ('porter', 'lancaster', 'snowball').\n",
        "#     Returns:\n",
        "#         List of stemmed tokens.\n",
        "#     \"\"\"\n",
        "#     if stemmer_type == 'porter':\n",
        "#         stemmer = PorterStemmer()\n",
        "#     elif stemmer_type == 'lancaster':\n",
        "#         stemmer = LancasterStemmer()\n",
        "#     elif stemmer_type == 'snowball':\n",
        "#         stemmer = SnowballStemmer(\"english\")\n",
        "#     else:\n",
        "#         raise ValueError(\"Invalid stemmer_type. Choose from 'porter', 'lancaster', 'snowball'.\")\n",
        "\n",
        "#     return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# # Example usage:\n",
        "# # text = \"running flies quickly ran\"\n",
        "# # tokens = text.split()\n",
        "# # stemmed_tokens = stem_english_text(tokens, stemmer_type='porter')\n",
        "# # print(stemmed_tokens)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A6HYghzomedo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title costum stop word removal\n",
        "# Custom stopwords (less aggressive for abstractive)\n",
        "custom_stopwords = set(stopwords.words('english')) - {'not', 'no', 'nor', 'only'}\n",
        "custom_stopwords.update(['cnn', 'daily mail', 'published', 'said'])\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token not in custom_stopwords]"
      ],
      "metadata": {
        "id": "SfZ6O0I4kQOy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pipelines**"
      ],
      "metadata": {
        "id": "TNi_1I5n-pLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preprocessing Pipeline for Traditional ML Algorithms and RNN Models\n",
        "\n",
        "def clean_for_classic(df, feature_name, is_arabic):\n",
        "    df = remove_rare_unique_chars(df, feature_name)\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for text in df[feature_name].astype(str):\n",
        "        text = remove_emojis(text)\n",
        "        if is_arabic:\n",
        "            text = normalize_foreign_tokens(text)\n",
        "            text = normalize_arabic(text)\n",
        "            tokens = simple_word_tokenize(text)\n",
        "        else:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            tokens = lemmatize_english_text(tokens)\n",
        "\n",
        "        tokens = remove_stopwords(tokens)\n",
        "        cleaned_tokens.append(tokens)\n",
        "\n",
        "    df[feature_name] = cleaned_tokens\n",
        "    return df\n",
        "\n",
        "df_arabic = clean_for_classic(df_arabic, \"text\", True)\n",
        "df_arabic = clean_for_classic(df_arabic, \"summary\", True)"
      ],
      "metadata": {
        "id": "iJ89MSmx7HnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preprocessing Pipeline for Deep Learning Algorithms and Transformer Models\n",
        "\n",
        "def clean_for_transformers_deep(df, feature_name, is_arabic):\n",
        "    \"\"\"\n",
        "    Cleans and normalizes a text column for deep learning and transformer models.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "        feature_name (str): Column name to clean.\n",
        "        is_arabic (bool): Whether the text is in Arabic.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Cleaned DataFrame with the same column name.\n",
        "    \"\"\"\n",
        "    cleaned_texts = []\n",
        "\n",
        "    for text in df[feature_name].astype(str):\n",
        "        text = remove_emojis(text)\n",
        "        if is_arabic:\n",
        "            text = normalize_foreign_tokens(text)\n",
        "            text = normalize_arabic(text)\n",
        "        else:\n",
        "            text = text.lower()\n",
        "        cleaned_texts.append(text)\n",
        "\n",
        "    df[feature_name] = cleaned_texts\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "dX3aIM877Lj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_for_deep_model(df, text_col, summary_col, max_len_input=100, max_len_summary=30, vocab_size=5000):\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "    texts = df[text_col].astype(str).values\n",
        "    summaries = df[summary_col].astype(str).values\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(np.concatenate((texts, summaries)))\n",
        "\n",
        "    X = tokenizer.texts_to_sequences(texts)\n",
        "    y = tokenizer.texts_to_sequences(summaries)\n",
        "\n",
        "    X = pad_sequences(X, maxlen=max_len_input, padding='post', truncating='post')\n",
        "    y = pad_sequences(y, maxlen=max_len_summary, padding='post', truncating='post')\n",
        "\n",
        "    return X, y, tokenizer"
      ],
      "metadata": {
        "id": "WB4ZB_B1GiFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models**"
      ],
      "metadata": {
        "id": "cFLdgCbL1zfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embedding_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "# Make sure your dataframes are properly loaded\n",
        "print(f\"English dataset shape: {df_english.shape}\")\n",
        "print(f\"Arabic dataset shape: {df_arabic.shape}\")"
      ],
      "metadata": {
        "id": "3dv1oYZmWmrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "def evaluate_language_dataset(df, is_arabic=False, language_name=\"English\"):\n",
        "    \"\"\"Evaluates models on a dataset with train/test reports for a single language\"\"\"\n",
        "    # Prepare dataset\n",
        "    text_col = 'text' if is_arabic else 'article'\n",
        "    summary_col = 'summary' if is_arabic else 'highlights'\n",
        "\n",
        "    df = df.dropna().head(500)\n",
        "    df['sentences'] = df[text_col].astype(str).apply(sent_tokenize)\n",
        "    df[summary_col] = df[summary_col].astype(str)\n",
        "\n",
        "    data = []\n",
        "    for _, row in df.iterrows():\n",
        "        sentences = [clean_for_transformers(s, is_arabic) for s in row['sentences']]\n",
        "        highlight = clean_for_transformers(row[summary_col], is_arabic)\n",
        "\n",
        "        sent_embeddings = sentence_embedding_model.encode(sentences)\n",
        "        hl_embedding = sentence_embedding_model.encode([highlight])[0]\n",
        "\n",
        "        similarities = cosine_similarity(sent_embeddings, [hl_embedding]).flatten()\n",
        "        labels = (similarities >= 0.5).astype(int)\n",
        "\n",
        "        for i, sent in enumerate(sentences):\n",
        "            data.append({'sentence': sent, 'embedding': sent_embeddings[i], 'label': labels[i]})\n",
        "\n",
        "    # Prepare train/test\n",
        "    df_sentences = pd.DataFrame(data)\n",
        "    X = np.stack(df_sentences['embedding'].values)\n",
        "    y = df_sentences['label'].values\n",
        "\n",
        "    # Dimensionality reduction for tree-based models\n",
        "    n_components = min(50, X.shape[1])  # Ensure we don't exceed available features\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_reduced = pca.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    X_train_reduced, X_test_reduced = train_test_split(\n",
        "        X_reduced, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Define models with separate processing paths\n",
        "    models = {\n",
        "        'Logistic Regression': {\n",
        "            'model': LogisticRegression(\n",
        "                penalty='l2', C=0.1, max_iter=1000,\n",
        "                class_weight='balanced', solver='liblinear'\n",
        "            ),\n",
        "            'use_pca': False\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'model': RandomForestClassifier(\n",
        "                n_estimators=100, max_depth=10,\n",
        "                min_samples_leaf=5, max_features='sqrt',\n",
        "                class_weight='balanced', random_state=42,\n",
        "                n_jobs=-1  # Enable parallel processing\n",
        "            ),\n",
        "            'use_pca': False\n",
        "        },\n",
        "        'SVM': {\n",
        "            'model': SVC(\n",
        "                kernel='linear', C=0.1,\n",
        "                class_weight='balanced', probability=True\n",
        "            ),\n",
        "            'use_pca': False\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'model': XGBClassifier(\n",
        "                eval_metric='logloss',\n",
        "                max_depth=4, reg_alpha=0.1, reg_lambda=1,\n",
        "                subsample=0.8, colsample_bytree=0.8,\n",
        "                learning_rate=0.05, n_estimators=100,\n",
        "                random_state=42\n",
        "            ),\n",
        "            'use_pca': False\n",
        "        },\n",
        "        'MLP': {\n",
        "            'model': MLPClassifier(\n",
        "                hidden_layer_sizes=(128, 64), alpha=0.001,\n",
        "                max_iter=300, early_stopping=True,\n",
        "                learning_rate_init=0.001, random_state=42\n",
        "            ),\n",
        "            'use_pca': False\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"🌍 {language_name.upper()} DATASET EVALUATION\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for name, config in models.items():\n",
        "        model = config['model']\n",
        "        use_pca = config['use_pca']\n",
        "\n",
        "        print(f\"\\n🔹 Model: {name}\")\n",
        "\n",
        "        # Select appropriate data version\n",
        "        if use_pca:\n",
        "            train_X = X_train_reduced\n",
        "            test_X = X_test_reduced\n",
        "            print(\"(Using PCA-reduced features)\")\n",
        "        else:\n",
        "            train_X = X_train\n",
        "            test_X = X_test\n",
        "            print(\"(Using full-dimensional embeddings)\")\n",
        "\n",
        "        model.fit(train_X, y_train)\n",
        "\n",
        "        print(\"\\nTRAIN SET PERFORMANCE:\")\n",
        "        train_pred = model.predict(train_X)\n",
        "        print(classification_report(y_train, train_pred))\n",
        "\n",
        "        print(\"\\nTEST SET PERFORMANCE:\")\n",
        "        test_pred = model.predict(test_X)\n",
        "        print(classification_report(y_test, test_pred))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "    return models  # Return trained models for further use"
      ],
      "metadata": {
        "id": "zY-88VlQNDUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bilingual_models(df_english, df_arabic):\n",
        "    \"\"\"Evaluates models on both English and Arabic datasets and returns them\"\"\"\n",
        "    print(\"\\n🔸 Evaluating English Models...\")\n",
        "    models_english = evaluate_language_dataset(df_english, is_arabic=False, language_name=\"English\")\n",
        "\n",
        "    print(\"\\n🔸 Evaluating Arabic Models...\")\n",
        "    models_arabic = evaluate_language_dataset(df_arabic, is_arabic=True, language_name=\"Arabic\")\n",
        "\n",
        "    return models_english, models_arabic\n"
      ],
      "metadata": {
        "id": "2cZHKcZNXqBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # @title Machine Learning Models\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.neural_network import MLPClassifier\n",
        "# from xgboost import XGBClassifier\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# import nltk\n",
        "\n",
        "\n",
        "# # -------------------------------\n",
        "# # 🔹 Step 1: Load SentenceTransformer (Multilingual for Arabic/English)\n",
        "# # -------------------------------\n",
        "# sentence_embedding_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "# # -------------------------------\n",
        "# # 🔹 Step 2: Prepare Sentence-Level Dataset\n",
        "# # -------------------------------\n",
        "# df_english = df_english.dropna().head(500)  # sample for speed\n",
        "\n",
        "# df_english['sentences'] = df_english['article'].astype(str).apply(sent_tokenize)\n",
        "# df_english['highlights'] = df_english['highlights'].astype(str)\n",
        "\n",
        "# data = []\n",
        "\n",
        "# for _, row in df_english.iterrows():\n",
        "#     sentences = row['sentences']\n",
        "#     highlight = row['highlights']\n",
        "\n",
        "#     sentence_embeddings = model.encode(sentences)\n",
        "#     highlight_embedding = model.encode([highlight])[0]\n",
        "\n",
        "#     similarities = cosine_similarity(sentence_embeddings, [highlight_embedding]).flatten()\n",
        "#     labels = (similarities >= 0.5).astype(int)\n",
        "\n",
        "#     for i, sent in enumerate(sentences):\n",
        "#         data.append({'sentence': sent, 'embedding': sentence_embeddings[i], 'label': labels[i]})\n",
        "\n",
        "# df_sentences = pd.DataFrame(data)\n",
        "# X = np.stack(df_sentences['embedding'].values)\n",
        "# y = df_sentences['label'].values\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # -------------------------------\n",
        "# # 🔹 Step 3: Define Models\n",
        "# # -------------------------------\n",
        "# models = {\n",
        "#     'Logistic Regression': LogisticRegression(\n",
        "#         penalty='l2',\n",
        "#         C=0.1,\n",
        "#         max_iter=1000,\n",
        "#         class_weight='balanced',\n",
        "#         solver='liblinear'\n",
        "#     ),\n",
        "\n",
        "#     'Random Forest': RandomForestClassifier(\n",
        "#         n_estimators=100,\n",
        "#         max_depth=10,\n",
        "#         min_samples_leaf=5,\n",
        "#         max_features='sqrt',\n",
        "#         class_weight='balanced',\n",
        "#         random_state=42\n",
        "#     ),\n",
        "\n",
        "#     'SVM': SVC(\n",
        "#         kernel='linear',\n",
        "#         C=0.1,\n",
        "#         class_weight='balanced',\n",
        "#         probability=True  # optional if you need predict_proba\n",
        "#     ),\n",
        "\n",
        "#     'XGBoost': XGBClassifier(\n",
        "#         use_label_encoder=False,\n",
        "#         eval_metric='logloss',\n",
        "#         max_depth=4,\n",
        "#         reg_alpha=0.1,\n",
        "#         reg_lambda=1,\n",
        "#         subsample=0.8,\n",
        "#         colsample_bytree=0.8,\n",
        "#         learning_rate=0.05,\n",
        "#         n_estimators=100,\n",
        "#         random_state=42\n",
        "#     ),\n",
        "\n",
        "#     'MLP': MLPClassifier(\n",
        "#         hidden_layer_sizes=(128, 64),\n",
        "#         alpha=0.001,\n",
        "#         max_iter=300,\n",
        "#         early_stopping=True,\n",
        "#         learning_rate_init=0.001,\n",
        "#         random_state=42\n",
        "#     )\n",
        "# }\n",
        "\n",
        "# # -------------------------------\n",
        "# # 🔹 Step 4: Train & Evaluate\n",
        "# # -------------------------------\n",
        "# for name, model in models.items():\n",
        "#     print(f\"\\n🔹 Model: {name}\")\n",
        "#     model.fit(X_train, y_train)\n",
        "#     y_pred = model.predict(X_test)\n",
        "#     print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "7Ntphpf_k3vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for name, model in models.items():\n",
        "#     print(f\"\\n🔹 Model: {name}\")\n",
        "#     model.fit(X_train, y_train)\n",
        "\n",
        "#     # Predict on train set\n",
        "#     y_train_pred = model.predict(X_train)\n",
        "#     print(\"Train Classification Report:\")\n",
        "#     print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "#     # Predict on test set\n",
        "#     y_test_pred = model.predict(X_test)\n",
        "#     print(\"Test Classification Report:\")\n",
        "#     print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "1hB8AC3pKZM7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# Evaluation Metrics**"
      ],
      "metadata": {
        "id": "2GgBDEdOjzAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROUGE: Overlap of words/phrases between your generated summary and a human-written reference summary."
      ],
      "metadata": {
        "id": "09gXN1MijzAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy nltk rouge-score"
      ],
      "metadata": {
        "id": "pWzrTfdTjzAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BLUE**: How \"natural\" your generated summary sounds compared to human references.\n"
      ],
      "metadata": {
        "id": "ofhBT_pbjzAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def calculate_rouge(generated_summary, reference_summary, n=1):\n",
        "    scorer = rouge_scorer.RougeScorer([f'rouge{n}'], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, generated_summary)\n",
        "    return scores[f'rouge{n}']\n",
        "\n",
        "def calculate_bleu(generated_summary, reference_summary):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu([reference_summary.split()], generated_summary.split(), smoothing_function=smoothie)"
      ],
      "metadata": {
        "id": "YCJtyIABjzAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_summaries_with_rouge_bleu(df_english, models_english):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    cumulative_scores = {\n",
        "        'rouge1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0},\n",
        "        'rouge2': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0},\n",
        "        'rougeL': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "    }\n",
        "    cumulative_bleu = 0.0\n",
        "\n",
        "    random_indices = random.sample(range(len(df_english)), 10)\n",
        "\n",
        "    for idx, article_idx in enumerate(random_indices, start=1):\n",
        "        try:\n",
        "            article_row = df_english.iloc[article_idx]\n",
        "            article_text = str(article_row['article'])\n",
        "            reference_summary = str(article_row['highlights'])\n",
        "\n",
        "            article_sentences = sent_tokenize(article_text)\n",
        "            sentence_embeddings = sentence_embedding_model.encode(article_sentences)\n",
        "\n",
        "            # Use the MLP model\n",
        "            predicted_labels = models_english['MLP'].predict(sentence_embeddings)\n",
        "\n",
        "            generated_summary_sentences = [\n",
        "                article_sentences[i] for i, label in enumerate(predicted_labels) if label == 1\n",
        "            ]\n",
        "            generated_summary = \" \".join(generated_summary_sentences)\n",
        "\n",
        "            scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "            for key in cumulative_scores:\n",
        "                cumulative_scores[key]['precision'] += scores[key].precision\n",
        "                cumulative_scores[key]['recall'] += scores[key].recall\n",
        "                cumulative_scores[key]['f1'] += scores[key].fmeasure\n",
        "\n",
        "            bleu_score = calculate_bleu(generated_summary, reference_summary)\n",
        "            cumulative_bleu += bleu_score\n",
        "\n",
        "            # Print output\n",
        "            print(f\"\\n--- Article {idx} ---\")\n",
        "            print(\"\\nOriginal Article:\")\n",
        "            print(article_text[:500] + '...' if len(article_text) > 500 else article_text)\n",
        "\n",
        "            print(\"\\nReference Summary:\")\n",
        "            print(reference_summary)\n",
        "\n",
        "            print(\"\\nGenerated Summary:\")\n",
        "            print(generated_summary if generated_summary else \"[No summary sentences predicted]\")\n",
        "\n",
        "            print(\"\\nROUGE Scores:\")\n",
        "            for key, val in scores.items():\n",
        "                print(f\"{key.upper()} - Precision: {val.precision:.4f}, Recall: {val.recall:.4f}, F1: {val.fmeasure:.4f}\")\n",
        "\n",
        "            print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing article {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Average scores\n",
        "    print(\"\\n--- AVERAGE SCORES OVER 10 ARTICLES ---\")\n",
        "    for key in cumulative_scores:\n",
        "        avg_precision = cumulative_scores[key]['precision'] / 10\n",
        "        avg_recall = cumulative_scores[key]['recall'] / 10\n",
        "        avg_f1 = cumulative_scores[key]['f1'] / 10\n",
        "        print(f\"{key.upper()} - Avg Precision: {avg_precision:.4f}, Avg Recall: {avg_recall:.4f}, Avg F1: {avg_f1:.4f}\")\n",
        "\n",
        "    print(f\"Average BLEU Score: {cumulative_bleu / 10:.4f}\")"
      ],
      "metadata": {
        "id": "viJfX9BNENET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate both languages\n",
        "evaluate_generation_metrics(df_english, is_arabic=False, language_name=\"English\")\n",
        "evaluate_generation_metrics(df_arabic, is_arabic=True, language_name=\"Arabic\")"
      ],
      "metadata": {
        "id": "ma7d1lX_EPS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *METEOR* (Metric for Evaluation of Translation with Explicit ORdering): Like BLEU but more flexible: accounts for synonyms, stemming, and word order."
      ],
      "metadata": {
        "id": "1190ZscLjzAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')  # Downloads WordNet corpus\n",
        "nltk.download('omw-1.4')  # Optional: Open Multilingual WordNet (for non-English)"
      ],
      "metadata": {
        "id": "zVmiEmvdjzAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "reference = \"the cat sat on the mat\".split()\n",
        "candidate = \"a cat is sitting on a mat\".split()\n",
        "score = meteor_score([reference], candidate)\n",
        "print(f\"METEOR Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "f7LAWZ6gjzAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Evaluation Metrics"
      ],
      "metadata": {
        "id": "uJ8QWZLngpu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ROUGE scorer\n",
        "import random\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Storage for cumulative scores\n",
        "cumulative_scores = {\n",
        "    'rouge1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0},\n",
        "    'rouge2': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0},\n",
        "    'rougeL': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "}\n",
        "\n",
        "# Pick 10 random articles\n",
        "random_indices = random.sample(range(len(df_english)), 10)\n",
        "\n",
        "for idx, article_idx in enumerate(random_indices, start=1):\n",
        "    article_row = df_english.iloc[article_idx]\n",
        "    article_text = str(article_row['article'])\n",
        "    reference_highlights = str(article_row['highlights'])\n",
        "\n",
        "    # Tokenize article into sentences\n",
        "    article_sentences = sent_tokenize(article_text)\n",
        "\n",
        "    try:\n",
        "        # Generate sentence embeddings\n",
        "        sentence_embeddings = sentence_embedding_model.encode(article_sentences)\n",
        "\n",
        "        # Predict labels (0: Not Summary, 1: Summary)\n",
        "        predicted_labels = models['MLP'].predict(sentence_embeddings)\n",
        "\n",
        "        # Extract predicted summary sentences\n",
        "        generated_summary_sentences = [\n",
        "            article_sentences[i] for i, label in enumerate(predicted_labels) if label == 1\n",
        "        ]\n",
        "        generated_summary = \" \".join(generated_summary_sentences)\n",
        "\n",
        "        # Compute ROUGE scores\n",
        "        scores = scorer.score(reference_highlights, generated_summary)\n",
        "\n",
        "        # Accumulate scores for average computation\n",
        "        for rouge_key in cumulative_scores:\n",
        "            cumulative_scores[rouge_key]['precision'] += scores[rouge_key].precision\n",
        "            cumulative_scores[rouge_key]['recall'] += scores[rouge_key].recall\n",
        "            cumulative_scores[rouge_key]['f1'] += scores[rouge_key].fmeasure\n",
        "\n",
        "        # Output for this article\n",
        "        print(f\"\\n--- Article {idx} ---\")\n",
        "        print(\"\\nOriginal Article:\")\n",
        "        print(article_text[:500] + '...' if len(article_text) > 500 else article_text)\n",
        "\n",
        "        print(\"\\nReference Highlights (Human-Written):\")\n",
        "        print(reference_highlights)\n",
        "\n",
        "        print(\"\\nGenerated Summary (Model Prediction):\")\n",
        "        print(generated_summary if generated_summary else \"[No summary sentences predicted]\")\n",
        "\n",
        "        print(\"\\nROUGE Scores:\")\n",
        "        for key, val in scores.items():\n",
        "            print(f\"{key.upper()} - Precision: {val.precision:.4f}, Recall: {val.recall:.4f}, F1: {val.fmeasure:.4f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing article {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "# --- Average Scores Across All Articles ---\n",
        "print(\"\\n--- AVERAGE ROUGE SCORES OVER 10 ARTICLES ---\")\n",
        "for key in cumulative_scores:\n",
        "    avg_precision = cumulative_scores[key]['precision'] / 10\n",
        "    avg_recall = cumulative_scores[key]['recall'] / 10\n",
        "    avg_f1 = cumulative_scores[key]['f1'] / 10\n",
        "    print(f\"{key.upper()} - Avg Precision: {avg_precision:.4f}, Avg Recall: {avg_recall:.4f}, Avg F1: {avg_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "jEDj7Y2RjzAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ROUGE scorer\n",
        "from rouge_score import rouge_scorer\n",
        "import random\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Storage for cumulative scores\n",
        "cumulative_scores = {\n",
        "    'rouge1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0},\n",
        "    'rouge2': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0},\n",
        "    'rougeL': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "}\n",
        "\n",
        "# Pick 10 random articles from Arabic dataset\n",
        "random_indices = random.sample(range(len(df_arabic)), 10)\n",
        "\n",
        "for idx, article_idx in enumerate(random_indices, start=1):\n",
        "    article_row = df_arabic.iloc[article_idx]\n",
        "    article_text = str(article_row['article'])\n",
        "    reference_highlights = str(article_row['highlights'])\n",
        "\n",
        "    # Tokenize Arabic article into sentences (adjust tokenizer if needed)\n",
        "    try:\n",
        "        article_sentences = sent_tokenize(article_text)  # Replace if needed with arabic-specific tokenizer\n",
        "\n",
        "        # Generate sentence embeddings\n",
        "        sentence_embeddings = sentence_embedding_model.encode(article_sentences)\n",
        "\n",
        "        # Predict labels (0: Not Summary, 1: Summary)\n",
        "        predicted_labels = models['MLP'].predict(sentence_embeddings)\n",
        "\n",
        "        # Extract predicted summary sentences\n",
        "        generated_summary_sentences = [\n",
        "            article_sentences[i] for i, label in enumerate(predicted_labels) if label == 1\n",
        "        ]\n",
        "        generated_summary = \" \".join(generated_summary_sentences)\n",
        "\n",
        "        # Compute ROUGE scores\n",
        "        scores = scorer.score(reference_highlights, generated_summary)\n",
        "\n",
        "        # Accumulate scores\n",
        "        for rouge_key in cumulative_scores:\n",
        "            cumulative_scores[rouge_key]['precision'] += scores[rouge_key].precision\n",
        "            cumulative_scores[rouge_key]['recall'] += scores[rouge_key].recall\n",
        "            cumulative_scores[rouge_key]['f1'] += scores[rouge_key].fmeasure\n",
        "\n",
        "        # Output for this article\n",
        "        print(f\"\\n--- Article {idx} ---\")\n",
        "        print(\"\\nOriginal Arabic Article:\")\n",
        "        print(article_text[:500] + '...' if len(article_text) > 500 else article_text)\n",
        "\n",
        "        print(\"\\nReference Highlights (Human-Written):\")\n",
        "        print(reference_highlights)\n",
        "\n",
        "        print(\"\\nGenerated Summary (Model Prediction):\")\n",
        "        print(generated_summary if generated_summary else \"[No summary sentences predicted]\")\n",
        "\n",
        "        print(\"\\nROUGE Scores:\")\n",
        "        for key, val in scores.items():\n",
        "            print(f\"{key.upper()} - Precision: {val.precision:.4f}, Recall: {val.recall:.4f}, F1: {val.fmeasure:.4f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing article {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "# --- Average Scores Across All Arabic Articles ---\n",
        "print(\"\\n--- AVERAGE ROUGE SCORES OVER 10 ARABIC ARTICLES ---\")\n",
        "for key in cumulative_scores:\n",
        "    avg_precision = cumulative_scores[key]['precision'] / 10\n",
        "    avg_recall = cumulative_scores[key]['recall'] / 10\n",
        "    avg_f1 = cumulative_scores[key]['f1'] / 10\n",
        "    print(f\"{key.upper()} - Avg Precision: {avg_precision:.4f}, Avg Recall: {avg_recall:.4f}, Avg F1: {avg_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "8iajhd02CRZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Arabic article\n",
        "arabic_article =\"\"\"\n",
        "ذهبت اليوم إلى مركز المجتمع المحلي للمساعدة في حملة تنظيف نهاية الأسبوع. تم تنظيم الحملة من قبل مجموعة من طلاب المرحلة الثانوية الذين أرادوا جعل الحديقة أنظف وأكثر أمانًا للأطفال.\n",
        "بدأنا في وقت مبكر من الصباح حوالي الساعة 8 صباحًا، وقد أحضر الجميع أدواتهم الخاصة مثل المكانس والقفازات وأكياس القمامة. كان الفريق متحمسًا، وكان هناك موسيقى تُعزف في الخلفية أثناء عملنا.\n",
        "بحلول الظهر، كنا قد ملأنا أكثر من 20 كيس قمامة كبير. وجد أحد الطلاب دراجة قديمة مدفونة تحت بعض الشجيرات، وساعدنا في إخراجها ووضعها جانبًا لإعادة التدوير.\n",
        "بعد التنظيف، جلسنا تحت الشجرة الكبيرة في وسط الحديقة وتشاركنا الوجبات الخفيفة والماء. بدا الجميع متعبين لكنهم فخورون بما أنجزناه في بضع ساعات فقط.\n",
        "شكرنا قائد المجموعة وذكر أنهم يخططون لتنظيم فعاليات مماثلة كل شهر.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the article into sentences\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# If you need to handle Arabic sentence splitting better, you can use the `arabicstopwords` or `farasa` if available\n",
        "sentences = sent_tokenize(arabic_article)\n",
        "\n",
        "# Encode sentences using your model\n",
        "sentence_embeddings = sentence_embedding_model.encode(sentences)\n",
        "\n",
        "# Predict labels using the trained model (Random Forest, MLP, etc.)\n",
        "predicted_labels = models['RandomForest'].predict(sentence_embeddings)\n",
        "\n",
        "# Extract predicted summary\n",
        "summary_sentences = [\n",
        "    sentence for sentence, label in zip(sentences, predicted_labels) if label == 1\n",
        "]\n",
        "generated_summary = \" \".join(summary_sentences)\n",
        "\n",
        "print(\"🔹 ملخص المقال:\")\n",
        "print(generated_summary if generated_summary else \"[لم يتم التنبؤ بأي جملة ملخصة]\")"
      ],
      "metadata": {
        "id": "ec7fyqOCBi4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# English article\n",
        "english_article =\"\"\"\n",
        "The event was organized by a group of high school students who wanted to make the park area cleaner and safer for kids.\n",
        "\n",
        "We started early in the morning around 8 a.m., and everyone brought their own tools like brooms, gloves, and trash bags. The team was enthusiastic, and there was even music playing in the background while we worked.\n",
        "\n",
        "By noon, we had already filled more than 20 large garbage bags. One of the students found an old bicycle buried under some bushes, and we helped pull it out and set it aside to be recycled.\n",
        "\n",
        "After the cleanup, we sat under the big tree in the center of the park and shared snacks and water. Everyone looked tired but proud of the difference we made in just a few hours.\n",
        "\n",
        "The leader of the group thanked us all and mentioned that they plan to organize similar events every month.\n",
        "\"\"\"\n",
        "\n",
        "# 1. Split into sentences\n",
        "sentences = sent_tokenize(english_article)\n",
        "\n",
        "# 2. Get embeddings (assumes sentence_embedding_model is a valid encoder like SBERT)\n",
        "sentence_embeddings = sentence_embedding_model.encode(sentences)\n",
        "\n",
        "# 3. Predict which sentences belong in the summary\n",
        "predicted_labels = models['RandomForest'].predict(sentence_embeddings)\n",
        "\n",
        "# 4. Build the summary\n",
        "summary_sentences = [s for s, label in zip(sentences, predicted_labels) if label == 1]\n",
        "summary = \" \".join(summary_sentences)\n",
        "\n",
        "print(\"🔹 English Summary:\")\n",
        "print(summary if summary else \"[No summary sentences predicted]\")\n"
      ],
      "metadata": {
        "id": "-HNBDiYvB7hX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}